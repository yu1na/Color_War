"""
AI í† ë¡ ì ì‹œìŠ¤í…œ (LLM ê¸°ë°˜, CPU ê²½ëŸ‰ ë²„ì „)
í˜ë¥´ì†Œë‚˜ë¥¼ ë°˜ì˜í•´ ìƒˆë¡œìš´ ëŒ“ê¸€ ìŠ¤íƒ€ì¼ë¡œ í† ë¡  ìƒì„±
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from typing import Optional
import sys, os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from model.comment_persona_engine import CommentPersonaEngine
from models import Side, DebateMessage, AnalysisResult, DebateState


class AIDebater:
    """AI í† ë¡ ì (ê²½ëŸ‰ LLM ê¸°ë°˜)"""

    def __init__(self, side: Side, analysis: AnalysisResult, persona_engine: CommentPersonaEngine, llm_pipeline):
        self.side = side
        self.analysis = analysis
        self.persona_engine = persona_engine
        self.llm = llm_pipeline  # âœ… pipeline ê³µìœ 
        self.device = "cpu"

    def generate_response(self, state: DebateState, opponent_message: Optional[DebateMessage] = None) -> str:
        """í† ë¡  ì‘ë‹µ ìƒì„± (ê²½ëŸ‰ ëª¨ë¸ ê¸°ë°˜)"""
        side_str = "left" if self.side == Side.LEFT else "right"
        persona_prompt = self.persona_engine.get_persona_prompt(side_str)

        conversation = ""
        for msg in state.messages[-4:]:
            speaker = "ë‚˜" if msg.side == self.side else "ìƒëŒ€"
            conversation += f"{speaker}: {msg.content}\n"

        topic = state.current_topic or "ì •ì¹˜ ë…¼ìŸ"
        opponent_text = opponent_message.content if opponent_message else "ì´ ì‚¬ì•ˆì— ëŒ€í•´ ë„ˆì˜ ìƒê°ì€ ë­ì•¼?"

        prompt = f"""
{persona_prompt}

í˜„ì¬ ì£¼ì œ: {topic}

ìµœê·¼ ëŒ€í™”:
{conversation}
ìƒëŒ€: {opponent_text}
ë‚˜:"""

        try:
            result = self.llm(
                prompt,
                max_new_tokens=150,
                temperature=0.8,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.llm.tokenizer.eos_token_id
            )[0]["generated_text"]

            response = result[len(prompt):].strip()
            if len(response) > 200:
                response = response.split(".")[0] + "."
            return response or "ê·¸ ë¶€ë¶„ì€ ì¢€ ë” ìƒê°í•´ë´ì•¼ê² ë„¤ìš”."

        except Exception as e:
            print(f"âš  ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ ({self.side.name}): {e}")
            return "ìŒ... ë‹¤ì‹œ ìƒê°í•´ë³¼ê²Œìš”."


class DebaterManager:
    """í† ë¡ ì ê´€ë¦¬ (ê²½ëŸ‰ ëª¨ë¸ + LLM íŒŒì´í”„ë¼ì¸ ê³µìœ )"""

    def __init__(self, analysis: AnalysisResult, persona_engine: CommentPersonaEngine):
        self.analysis = analysis
        self.persona_engine = persona_engine

        # âœ… ê²½ëŸ‰ ëª¨ë¸ ì„¤ì •
        self.model_name = "skt/kogpt2-base-v2"
        self.device = "cpu"
        print(f"ğŸ¤– ëŒ€í™” ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name} ({self.device})")

        try:
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,
                device_map=None,
                low_cpu_mem_usage=True
            ).to(self.device)

            llm_pipeline = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=-1
            )

            print("âœ“ ëŒ€í™” ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (CPU ê²½ëŸ‰ ëª¨ë“œ)\n")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            llm_pipeline = None

        # ë‘ í† ë¡ ì ìƒì„±
        self.left_debater = AIDebater(Side.LEFT, analysis, persona_engine, llm_pipeline)
        self.right_debater = AIDebater(Side.RIGHT, analysis, persona_engine, llm_pipeline)

    def generate_response(self, side: Side, state: DebateState, opponent_message: Optional[DebateMessage] = None):
        """í† ë¡ ìë³„ ì‘ë‹µ ìƒì„±"""
        if side == Side.LEFT:
            return self.left_debater.generate_response(state, opponent_message)
        else:
            return self.right_debater.generate_response(state, opponent_message)
